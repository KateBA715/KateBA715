{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KateBA715/KateBA715/blob/main/cis2349cFinalProjectPySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcMPzJlMYPL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff32b83c-00dd-4fb6-e55f-e9aee5add209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=1c0ab7d919b8a22609a3921e05f1378b4519209c08c00b70a150a8b89f49278a\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "# CIS2349C Final Project\n",
        "\n",
        "# make sure pyspark is installed\n",
        "!pip install pyspark\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "g3E1DQsyGhw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8221dd83-7a66-4d7f-bc3b-96cb7c483c24"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e62dee04-0bdc-4136-8d3c-9efbbd6d30e4",
        "_uuid": "e8b261ed3bf92f4c7e26f6ef8c656381d7261e80",
        "collapsed": true,
        "id": "RbSCT-r_sRYj"
      },
      "outputs": [],
      "source": [
        "# import the important things\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType,\\\n",
        "                              StringType, DoubleType, BooleanType\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8Qca05uYo_y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "87cce088-49a0-4259-b0c6-12bcbb4d639d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c3649e09780>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://2f259b4d935c:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>cis2349c</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# create a Spark session entry point and create a cluster\n",
        "spark=SparkSession.builder.appName('cis2349c').getOrCreate()\n",
        "\n",
        "# create a cluster\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6GTEdv6ZLkH"
      },
      "source": [
        "In the next cell you will create a Spark dataframe (similar to, but different from, a Pandas dataframe) from the uploaded CSV data file (if you have not uploaded it yet, you should do so now). Note that the data file includes headers.\n",
        "\n",
        "Use the spark.read.csv('filename') command to read the data file. Setting the <b>header</b> option in the arguments to <b>True</b> will properly assign the header names. For example, assuming you name your Spark dataframe \"df_pyspark\", use the following command:\n",
        "\n",
        "<pre>df_pyspark = spark.read.csv('/content/cis2349cPaySim.csv', header=True)</pre>\n",
        "\n",
        "Use the \"Copy Path\" option from the Files tool (left vertical menu) to properly set the path for the file.\n",
        "\n",
        "Assign the dataframe to a variable for later use. In the code examples provided below the name 'df_pyspark' is used.\n",
        "\n",
        "To view the first few lines of the data, use the show function, e.g.\n",
        "\n",
        "<pre>df_pyspark.show(5)</pre>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt:\n",
        "\n",
        "# Load the data file into a Spark dataframe\n",
        "df_pyspark = spark.read.csv('/content/cis2349cPaySim.csv', header=True)\n",
        "\n",
        "# Print the schema of the dataframe\n",
        "df_pyspark.printSchema()\n",
        "\n",
        "# Show the first few rows of the dataframe\n",
        "df_pyspark.show(5)\n"
      ],
      "metadata": {
        "id": "v6aMEiydWBjr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0dc433-c7eb-4542-c996-2007e88b205f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- step: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            " |-- nameOrig: string (nullable = true)\n",
            " |-- oldbalanceOrg: string (nullable = true)\n",
            " |-- newbalanceOrig: string (nullable = true)\n",
            " |-- nameDest: string (nullable = true)\n",
            " |-- oldbalanceDest: string (nullable = true)\n",
            " |-- newbalanceDest: string (nullable = true)\n",
            " |-- isFraud: string (nullable = true)\n",
            " |-- isFlaggedFraud: string (nullable = true)\n",
            "\n",
            "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
            "|step|    type|  amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
            "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
            "|   1| PAYMENT| 9839.64|C1231006815|       170136|     160296.36|M1979787155|             0|             0|      0|             0|\n",
            "|   1| PAYMENT| 1864.28|C1666544295|        21249|      19384.72|M2044282225|             0|             0|      0|             0|\n",
            "|   1|TRANSFER|     181|C1305486145|          181|             0| C553264065|             0|             0|      1|             0|\n",
            "|   1|CASH_OUT|     181| C840083671|          181|             0|  C38997010|         21182|             0|      1|             0|\n",
            "|   1| PAYMENT|11668.14|C2048537720|        41554|      29885.86|M1230701703|             0|             0|      0|             0|\n",
            "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOzw6liafdZJ"
      },
      "source": [
        "Now let's clean up the data and do some transformations. The original file had some inconsistent column names, and we also want to use camel-case consistently for the headings. Let's change the following headings (look carefully at the names, the changes are subtle).\n",
        "\n",
        "- 'oldbalanceOrg' should be 'oldBalanceOrig'\n",
        "- 'newbalanceOrig' should be 'newBalanceOrig'\n",
        "- 'oldbalanceDest' should be 'oldBalanceDest'\n",
        "- 'newbalanceDest' should be 'newBalanceDest'\n",
        "\n",
        "To make these changes, we can use the PySpark \"withColumnRenamed\" function, e.g.\n",
        "\n",
        "<pre>df_pyspark.withColumnRenamed('oldbalanceOrg', 'oldBalanceOrig')</pre>\n",
        "\n",
        "These functions can be chained to help keep them logically organized, e.g.\n",
        "\n",
        "<pre>df_pyspark.withColumnRenamed('oldname1', 'newname1')\n",
        "              .withColumnRenamed('oldname2', 'newname2')</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JnI0AsuLfbQ2"
      },
      "outputs": [],
      "source": [
        "df_pyspark = (df_pyspark.withColumnRenamed('oldbalanceOrg', 'oldBalanceOrig')\n",
        "                       .withColumnRenamed('newbalanceOrig', 'newBalanceOrig')\n",
        "                       .withColumnRenamed('oldbalanceDest', 'oldBalanceDest')\n",
        "                       .withColumnRenamed('newBalanceDest', 'newBalanceDest'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chAP9aK-jBFc"
      },
      "source": [
        "You may have noticed that all of the columns in the dataframe have a type of string. We need to modify those types in order to process them further.\n",
        "\n",
        "The type changes needed should be as shown here:\n",
        "\n",
        "- step: integer\n",
        "- oldBalanceOrig: double\n",
        "- newBalanceOrig: double\n",
        "- oldBalanceDest: double\n",
        "- newBalanceDest: double\n",
        "- isFraud: boolean\n",
        "- flaggedAsFraud: boolean\n",
        "\n",
        "To change a data type, use the PySpark withColumn chained to a cast function, e.g.\n",
        "\n",
        "<pre>df_pyspark.withColumn('step', col('step').cast('integer')</pre>\n",
        "\n",
        "As with the withColumnRenamed function, you can chain multiples of these together, e.g.\n",
        "\n",
        "<pre>df_pyspark.withColumn('column1', col('column1').cast('type'))\\\n",
        "      .withColumn('column2', col('column2').cast('type'))</pre>\n",
        "\n",
        "The boolean columns are a little more complex, here is <b>isFraud</b> as an example:\n",
        "\n",
        "<pre>df_pyspark = df_pyspark.withColumn('isFraud',\n",
        "       when(col('isFraud').isin('1'), True)\n",
        "      .when(col('isFraud').isin('0'), False)\n",
        "      .otherwise(None))</pre>\n",
        "\n",
        "\n",
        "Finally, to verify the changes, use the print_schema function:\n",
        "\n",
        "<pre>df_pyspark.printSchema()</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "UX-8QiEihsJu"
      },
      "outputs": [],
      "source": [
        "df_pyspark = df_pyspark.withColumn('step', col('step').cast('integer'))\\\n",
        "    .withColumn('amount', col('amount').cast('double'))\\\n",
        "    .withColumn('oldBalanceOrig', col('oldBalanceOrig').cast('double'))\\\n",
        "    .withColumn('newBalanceOrig', col('newBalanceOrig').cast('double'))\\\n",
        "    .withColumn('oldBalanceDest', col('oldBalanceDest').cast('double'))\\\n",
        "    .withColumn('newBalanceDest', col('newBalanceDest').cast('double'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_pyspark = df_pyspark.withColumn('isFraud', col('isFraud').cast('string'))\\\n",
        "    .withColumn('isFraud',\n",
        "       when(col('isFraud').isin('1'), True)\n",
        "      .when(col('isFraud').isin('0'), False)\n",
        "      .otherwise(None))\\\n",
        "    .withColumn('isFlaggedFraud', col('isFlaggedFraud').cast('string'))\\\n",
        "    .withColumn('isFlaggedFraud',\n",
        "       when(col('isFlaggedFraud').isin('1'), True)\n",
        "      .when(col('isFlaggedFraud').isin('0'), False)\n",
        "      .otherwise(None))\n",
        "\n",
        "df_pyspark.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr6BLiAQDWr9",
        "outputId": "e8a1ec5e-b368-46ff-f3dd-544069ff5e72"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- step: integer (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- amount: double (nullable = true)\n",
            " |-- nameOrig: string (nullable = true)\n",
            " |-- oldBalanceOrig: double (nullable = true)\n",
            " |-- newBalanceOrig: double (nullable = true)\n",
            " |-- nameDest: string (nullable = true)\n",
            " |-- oldBalanceDest: double (nullable = true)\n",
            " |-- newBalanceDest: double (nullable = true)\n",
            " |-- isFraud: boolean (nullable = true)\n",
            " |-- isFlaggedFraud: boolean (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-PABSZtFMYW"
      },
      "source": [
        "In a distributed data environment (e.g. Hadoop) there are various ways to access data for analysis using PySpark. \"show\" is the most efficient for large datasets, we could also use \"collect\", \"take\", \"head\", but these functions will first retrieve the entire dataset (\"collect it to the driver\"). \"show\" only accesses what is needed to show the desired number of lines.\n",
        "\n",
        "Here’s what happens when you use collect() or similar methods (take(), head(), etc.):\n",
        "- Data Aggregation: The distributed data residing on multiple worker nodes is gathered together.\n",
        "- Data Transfer: This aggregated data is then transferred over the network to the driver node.\n",
        "- Single Location: The data is now available on the driver node as a local object (like a list or array in Python), and you can interact with it just like you would with any local data structure in Python.\n",
        "\n",
        "Using these methods has several implications:\n",
        "- Memory Limitation: The driver node has its own memory limits. If you try to collect a very large dataset, it might not fit into the driver’s memory, leading to errors or crashes.\n",
        "- Performance Impact: Collecting data from distributed nodes to a single node can be network and resource-intensive, especially for large datasets. It can significantly impact the performance and should be used judiciously.\n",
        "- Use Cases: It’s often used for smaller datasets or when the final output of a large computation is relatively small (like collecting the final results of an aggregation).\n",
        "\n",
        "count() is another method which triggers computation over the entire DataFrame. Depending on the size of your data and the configuration of your Spark cluster, this operation might take some time for very large datasets.\n",
        "\n",
        "In the following cell, show the row count of the dataframe (it should be 800000)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0FnIW3v9mLYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e31856e1-e08a-4ad5-aaa8-512441c4db02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row Count: 353714\n"
          ]
        }
      ],
      "source": [
        "row_count = df_pyspark.count()\n",
        "print('Row Count:', row_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sAqKI3qGiZr"
      },
      "source": [
        "According to the author of our reference material for this project, we want to do the fraud analysis based on transaction types that are transfers (type == TRANSER) or cash-out (type == CASH_OUT) where the account owner is withdrawing funds.\n",
        "\n",
        "As with Pandas, PySpark can filter dataframe rows based on logical conditions. In the following cell, filter the dataframe rows based on the transaction type as described above using a logical OR. Here is the filter command:\n",
        "\n",
        "<pre>df_pyspark = df_pyspark.filter((col(\"type\") == \"TRANSFER\") | (col(\"type\") == \"CASH_OUT\"))</pre>\n",
        "\n",
        "After running the filter, show the new row count of the dataframe (it should be 353714)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "_cell_guid": "bf2306d5-b48d-441c-b217-478ede54f36d",
        "_uuid": "8b773d7747aa7fee6cf563ce2e11a4b636279314",
        "collapsed": true,
        "id": "9LhRdpKhsRYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af0dc4d5-76ba-4e41-97df-fe641044c06a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row Count: 353714\n"
          ]
        }
      ],
      "source": [
        "df_pyspark = df_pyspark.filter((col(\"type\") == \"TRANSFER\") | (col(\"type\") ==\n",
        "                                                              \"CASH_OUT\"))\n",
        "\n",
        "row_count = df_pyspark.count()\n",
        "print('Row Count:', row_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zswc5ESGJDrm"
      },
      "source": [
        "For this exercise, we will use a simple heuristic (rule-based) approach of categorizing any large-value transactions as fraud. This approach is oversimplistic but is sufficient for our purposes.\n",
        "\n",
        "To do this we calculate the median value of the previously filtered transactions using the value of 1 (True) for the <b>isFraud</b> column and then using the  <b>approxQuantile</b> method (since PySpark does not have a built-in median function). We will use that value as the threshold for suspected fraud.\n",
        "\n",
        "Use the filter function provided in the previous cell to find the rows where isFraud == 1.\n",
        "\n",
        "Here is the syntax for the approxQuantile method:\n",
        "\n",
        "approxQuantile('column', [probability], relative error)[0]\n",
        "\n",
        "Call the approxQuantile method on the amount column, with the value of 0.5 for the median, and 0 for the relative error (0 is used for an exact computation). The [0] at the end of the argument list extracts the value from the list that is returned by the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "_cell_guid": "4eb02425-627c-4fd7-9368-fa1139d6a9ba",
        "_uuid": "1738dde2daeba1f39f8e625e34f1a1b701ba412a",
        "id": "THWYxAG0sRYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5126ce9-78ae-4e14-f794-f1dd7a845de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No fraudulent transactions found...this time.\n"
          ]
        }
      ],
      "source": [
        "fraud_dfps = df_pyspark.filter((df_pyspark.isFraud == 1) & (df_pyspark.type.isin\n",
        " ([\"TRANSFER\", \"CASH_OUT\"])))\n",
        "median_value = fraud_dfps.approxQuantile(\"amount\",[0.5],0)\n",
        "\n",
        "if len(median_value) > 0:\n",
        "    fraud_median = median_value[0]\n",
        "    print('Fraudulent Transactions-Median Value:', median_value)\n",
        "else:\n",
        "    print('No fraudulent transactions found...this time.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTljGxdaQVaD"
      },
      "source": [
        "Now let's create a boolean column indicating whether a row should be included in our Fraud Heuristic data.\n",
        "\n",
        "Use the \"withColumn\" method and a \"when\" expression specifying an amount > the calculated median. Set the column value to 1 for this condition, 0 otherwise:\n",
        "\n",
        "<pre>df_pyspark.withColumn(\"Fraud_Heuristic\", when(col(\"amount\") > median_value, 1).otherwise(0))</pre>\n",
        "\n",
        "After applying this operation, show a selection of columns for the first few rows to verify:\n",
        "\n",
        "<pre>df_pyspark.select(\"amount\", \"Fraud_Heuristic\").show()</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "_cell_guid": "02d2d83a-508d-48ba-9c6d-52e104c7f9ae",
        "_uuid": "8b925d89ed3a57de62b217beeb8271bd658bf028",
        "collapsed": true,
        "id": "Ug0YxIDxsRYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0382cec-27e0-4826-ea04-de90ee56b310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No fraudulent transactions found...this time.\n"
          ]
        }
      ],
      "source": [
        "fraud_dfps_count = fraud_dfps.count()\n",
        "if fraud_dfps_count > 0:\n",
        "    median_value = fraud_dfps.approxQuantile(\"amount\",[0.5],0)\n",
        "    fraud_median = median_value[0]\n",
        "    print('Fraudulent Transactions-Median Value:', fraud_median)\n",
        "else:\n",
        "    print('No fraudulent transactions found...this time.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7wiZc3EV4vx"
      },
      "source": [
        "Now let's calculate an F1 score using PySpark's <b>MulticlassClassificationEvaluator</b>, part of Apache Spark's MLlib machine learning library. It is used to evaluate the performance of classification models on a dataset.\n",
        "\n",
        "The F1 score is a measure of a model's accuracy. This score is particularly useful when false positives and false negatives carry different costs (e.g., a false positive for fraud can cause you to lose customers!)\n",
        "\n",
        "The score measures precision and recall.\n",
        "\n",
        "- <b>Precision</b> is the ratio of correctly predicted positive observations to the total predicted positives. It answers the question, \"Of all the labels predicted as positive, how many are actually positive?\"\n",
        "- <b>Recall</b> is the ratio of correctly predicted positive observations to  all observations in the actual class. It answers the question, \"Of all the actual positives, how many did we predict as positive?\"\n",
        "\n",
        "Values of the F1 score range from 0 to 1, where 1 indicates perfect precision and recall; the higher the F1 score, the better.\n",
        "\n",
        "To set up the call to the evaluator, use the following code (assuming your PySpark dataframe is named \"df_pyspark\")\n",
        "\n",
        "<pre># Ensure that the columns are of type Double,\n",
        "# required by the MulticlassClassificationEvaluator\n",
        "df_pyspark = df_pyspark.withColumn(\"Fraud_Heuristic\", col(\"Fraud_Heuristic\")\n",
        "                        .cast(DoubleType())) \\\n",
        "                        .withColumn(\"isFraud\", col(\"isFraud\").cast(DoubleType()))\n",
        "\n",
        "# Create an evaluator for binary classification with the metric as F1\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"isFraud\",\n",
        "                                              predictionCol=\"Fraud_Heuristic\",\n",
        "                                              metricName=\"f1\")\n",
        "</pre>\n",
        "\n",
        "The evaluator constructor arguments are as follows:\n",
        "\n",
        "- labelCol=\"isFraud\": This parameter specifies the name of the column in the dataset that contains the true labels for the classification task\n",
        "- predictionCol=\"Fraud_Heuristic\": This parameter indicates the name of the column that contains the predictions made by the classification model.\n",
        "- metricName=\"f1\": This parameter specifies the metric used to evaluate the model's performance. The \"f1\" metric refers to the F1 score.\n",
        "\n",
        "To actually run the evaluator and calculate the F1 score, run the following:\n",
        "\n",
        "<pre># Calculate the F1 score\n",
        "f1_score = evaluator.evaluate(df_pyspark)\n",
        "</pre>\n",
        "\n",
        "and then you can just print the f1_score.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrmlVvLWauT6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSQhh4jnj8A9"
      },
      "source": [
        "If you calculated this score correctly, you should see a value of about 2/3 (0.66...).\n",
        "\n",
        "This F1 score could be interpreted as either good or not so good depending on the context of the specific problem, the complexity of the task, the nature of the data, and the baseline or comparison scores.\n",
        "\n",
        "In domains such as medical diagnosis or fraud detection, a higher F1 score is generally desired due to the critical nature of false negatives and false positives. In such cases, an F1 score of 66% might be considered insufficient."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}